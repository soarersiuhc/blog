---
layout: post
title: "CIFAR-10 #1 - kNN"
page_name: 2020-05-17-CIFAR-10--kNN
date: 2020-05-17
categories: data-science cs231n
---

To get myself through finishing [CS231n](https://cs231n.github.io/) (finally!), it could be interesting to just play with all the techniques covered in that class, on the [CIFAR-10 data set](https://www.cs.toronto.edu/~kriz/cifar.html). 

In this first post, I am going to cover **k-nearest neighbors (kNN)**. This is impractical for most purposes - some cons I can think of are:
- It is very slow once your data set gets bigger.
  - It's worth mentioning that suitable data structure such as [kd tree](https://en.wikipedia.org/wiki/K-d_tree) can speed this up, though I did not try to benchmark this.
- The premise of kNN is that two inputs that are "close" should have (largely the) same label. When this is violated too much, kNN is useless. For example,
  - Metric chosen is bad. Euclidean metric is the usual metric we pick, and
    - Input may have to be rescaled. Otherwise components with highest variation for distance will dominate.
    - Dimension cannot be too high. As dimension increases, the ratio between the nearest and farthest points approaches 1 in probability, and so "nearest neighbors" will be more and more ill-defined as dimension goes up. (An example of curse of dimensionality).
  - Source data is too imbalanced. kNN depends on majority voting for its nearest neighbors. So if there are too few neighbors of the correct label, just because of the data imbalance, then performance of kNN is affected. (Or that you will be forced to choose a very small $k$, that makes it more prone to overfitting).

## Applying kNN to CIFAR-10
Let us first import CIFAR-10. To speed things up, we will only use 5000 images in training set and 500 images in the test set.
```python
import tensorflow as tf
import matplotlib.pyplot as plt
import numpy as np

(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()

X_train.shape, y_train.shape, X_test.shape, y_test.shape
```
  ((50000, 32, 32, 3), (50000, 1), (10000, 32, 32, 3), (10000, 1))
```python
n_train = 5000
n_test = 500

def get_mask(n_total, n_sample):
  mask = np.zeros(n_total)
  mask[:n_sample] = 1
  np.random.shuffle(mask)

  return mask

train_mask = get_mask(len(X_train), n_train)
test_mask = get_mask(len(X_test), n_test)

X_train_sampled, y_train_sampled = X_train[train_mask == 1], y_train[train_mask == 1]
X_test_sampled, y_test_sampled = X_test[test_mask == 1], y_test[test_mask == 1]

X_train_sampled.shape, y_train_sampled.shape, X_test_sampled.shape, y_test_sampled.shape
```

   ((5000, 32, 32, 3), (5000, 1), (500, 32, 32, 3), (500, 1))

---


Here is a simple implementation of kNN. It allows both $L^1$ and $L^2$ norm - just so I can compare performance difference - and would evaluate the accuracy of predictions.
```python
class KNN:
  def __init__(self, *, k, dist=None):
    self.k = k
    self.dist = dist or "l1"

    assert self.dist in ["l1", "l2"]

    print(f"Using distance: {self.dist}")

  def fit(self, X_train, y_train):
    self.X_train = X_train
    self.y_train = y_train

  def predict(self, X_batch):
    X_batch_expanded = np.expand_dims(X_batch, 1)

    delta = np.abs(self.X_train - X_batch_expanded)
    if self.dist == "l1":
      distance = np.sum(delta, axis=(2, 3, 4))
    elif self.dist == "l2":
      distance = np.sqrt(np.sum(delta ** 2, axis=(2, 3, 4)))
    else:
      raise NotImplementedError(f"Unsupported distance function! dist: {dist}")

    closest_k_indices = np.argsort(distance, axis=1)[:, :self.k]
    closest_k_labels = np.take_along_axis(
        self.y_train.reshape(1, -1),
        closest_k_indices,
        axis=1
    )

    def get_most_frequent(arr):
      bincount = np.bincount(arr)
      return np.argmax(bincount)

    pred = np.apply_along_axis(
        get_most_frequent, 1, closest_k_labels
    ).reshape(-1)
    ret = pred.reshape(-1)

    return ret

  def evaluate(self, X_test, y_test, batch_size=100, quiet=False):
    n_sample = X_test.shape[0]
    n_correct = 0

    batch_idx = 0
    n_evaluated = 0
    while batch_idx < n_sample:
      X_batch, y_batch = X_test[batch_idx:batch_idx + batch_size], y_test.reshape(-1)[batch_idx:batch_idx + batch_size]

      pred_batch = self.predict(X_batch)

      n_evaluated += len(X_batch)
      n_correct += sum(pred == label for pred, label in zip(pred_batch, y_batch))

      if not quiet:
        print(f"Evaluated {n_evaluated} samples, {n_correct} labels are correct. Accuracy: {n_correct / n_evaluated}")

      batch_idx += batch_size

    accuracy = n_correct / n_evaluated
    return accuracy
```

Now let's try it out! We start with 1-NN with $L^1$-norm to make sure things look okay.
```python
l1_mod = KNN(k=1)
l1_mod.fit(X_train_sampled, y_train_sampled)
```

Using distance: l1

```python
l1_mod.predict(X_test_sampled[:3])
```

array([1, 0, 9])

```python
l1_mod.evaluate(X_test_sampled, y_test_sampled, batch_size=100)
```

Evaluated 100 samples, 23 labels are correct. Accuracy: 0.23
Evaluated 200 samples, 50 labels are correct. Accuracy: 0.25
Evaluated 300 samples, 78 labels are correct. Accuracy: 0.26
Evaluated 400 samples, 99 labels are correct. Accuracy: 0.2475
Evaluated 500 samples, 114 labels are correct. Accuracy: 0.228
0.228

---


Great! Things seem to work. Let's compare the performance of kNN as $k$ varies, and norms change ($L^1$ or $L^2)$

```
list_k = list(range(1, 11))
```
```
l1_eval = []

for k in list_k:
  mod = KNN(k=k, dist="l1")
  mod.fit(X_train_sampled, y_train_sampled)

  accuracy = mod.evaluate(X_test_sampled, y_test_sampled, batch_size=100, quiet=True)
  l1_eval.append(accuracy)
```
```
l2_eval = []

for k in list_k:
  mod = KNN(k=k, dist="l2")
  mod.fit(X_train_sampled, y_train_sampled)

  accuracy = mod.evaluate(X_test_sampled, y_test_sampled, batch_size=100, quiet=True)
  l2_eval.append(accuracy)
```
```
fig, ax = plt.subplots()

ax.plot(list_k, l1_eval, label="l1 distance")
ax.plot(list_k, l2_eval, label="l2 distance")
ax.set(
    title="Accuracy of kNN",
    xlabel="k",
    ylabel="Accuracy"
)

plt.legend()
plt.show()
```

![png]({{site.baseurl}}/assets/{{page.page_name}}/eval.png)

---


So it looks like 1-NN with $L^1$ distance works best here. The accuracy is quite dismal though: we beat the baseline of random labels (0.1, since there are 10 labels), but [state of the art can do >99% on CIFAR-10.](https://benchmarks.ai/cifar-10). This is not unexpected, given that

- Euclidean distance between RGB seems fairly meaningless. Two pictures are likely the "same" if we rotate it, adjust the color/saturation/hue, change foreground/background - but all these are considered different under euclidean distance between RGB pixels.
- The dimension of each picture $3072 = 32 \times 32 \times 3$ is likely too high, and curse of dimensionality already kicks in.


## The Math: kNN from a Bayesian viewpoint

kNN can be considered a discriminative model, where we model $p(x\vert \text{class = i})$.

Let's start with a more generic situation. Suppose $X_1,\cdots,X_n$ are i.i.d. samples of the probability density $p(x)$ on $\mathbb{R}^N$. Fix a point $x_0$ - we wish to estimate $p(x_0)$. Consider a small neighborhood $R$ around $x_0$. Then the probability of drawing points in $R$ is 

$$\mathbb{P}\left(X \sim p, X \in R \right) = \int_R p(x) dx$$

- On one hand, if we assume that $f$ is basically constant on $R$, then $\int_R p(x) dx \approx p(x_0) vol(R)$.
- On the other hand, since we already have $n$ samples from $p$, we may also estimate the probability by $\frac{\text{Number of sampled points in $R$}}{n}$.

Thus we have

$$p(x_0) vol(R) \approx \int_R p(x) dx \approx \frac{\text{Number of sampled points in $R$}}{n} \Rightarrow p(x_0) \approx \frac{\text{Number of sampled points in $R$}}{n \cdot vol(R)}$$

Let's specialize the situation to the classification problem at hand. Let us fix $k$, and assume that for $i = 1, \cdots, k$, we have probability density $p(x \vert \text{class = $i$}) := p_i(x)$ of drawing a point of class $i$. Assume that

- Each point must have a label, i.e. $\sum_i p_i(x) = 1$.
- For each $x$, each density $p_i(x)$ is basically constant on the small neighborhood containing $k$ nearest points to $x$.
- Number of samples in each class is big enough, so that $\frac{\text{Number of class $i$ points in $R$}}{\text{Number of class $i$ points}}$ is a good estimate of $\mathbb{P}\left(X \sim p_i(x), X \in R\right)$

Then both probability estimates above hold, meaning that for fixed $x_0$ and class $i$,

$$p_i(x_0) vol(R) \approx \int_R p_i(x) dx \approx \frac{k_i}{n_i}$$

Here $n_i$ is the total number of points of class $i$, and $k_i$ is the number of points of class $i$, among the $k$ nearest points to $x_0$. In particular this implies

$$p_i(x_0) \approx \frac{k_i}{n_i vol(R)}$$

Note also that $p(\text{class = i})$ can be estimated by $\frac{n_i}{n}$, where $n = \sum n_i$ is the total number of points. By Bayes' theorem, we thus have

$$p(\text{class = i} \vert x) = \frac{p_i(x) p(\text{class = i})}{p(x)} = \frac{p_i(x) p(\text{class = i})}{ \sum_i p_i(x) p(\text{class = i})} \approx \frac{\frac{k_i}{n_i vol(R)} \cdot \frac{n_i}{n}}{\sum \frac{k_i}{n_i vol(R)} \cdot \frac{n_i}{n}} = \frac{k_i}{\sum k_i} = \frac{k_i}{k}$$

This is exactly the kNN algorithm!

**Remark** It is interesting to recover $p(x)$ based on finite samples - this is the problem of [kernel density estimation](https://en.wikipedia.org/wiki/Kernel_density_estimation). Our method above gives us a way to estimate the density, if we take $R$ to be the smallest closest ball containing the $k$ nearest neighbors of $x_0$. This is not a good method however, because

- Tail may be too heavy, since estimated density decays like $\frac{1}{vol(\text{ ball of radius $x$})}$ as $x \to \infty$. In one dimension the resulting estimate is not even a density because of this.
- Resulting density is very discontinuous - discontinuity always happens in "transition regions" of nearest neighbors.

(See e.g. [here](http://www.cs.haifa.ac.il/~rita/ml_course/lectures/KNN.pdf for illustration of resulting estimates) In any case, it is still of interest to know the basic quality measures of the resulting estimates: bias an variance. For 1 dimension it's documented [here](http://faculty.washington.edu/yenchic/18W_425/Lec7_knn_basis.pdf). Based on that the optimal $k$ should be of the scale $n^{4/5}$, although it seems like the rule of thumb is $\sqrt{n}$ from multiple sources I have seen. In practice though, best $k$ is likely obtained by cross validation instead.
